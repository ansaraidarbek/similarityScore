{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Ranking </h1>\n",
    "Task: create dataset based on the unique pairs of 11 movies from the imdb dataset, annotate each row with preference vote, compare the result of Bradleyâ€“Terry model output with the original dataset<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Simlex999 data</h2>\n",
    "    This dataset consist of 1000 entries, each having 8 columns  <br>\n",
    "    <hr>\n",
    "    <p style=\"background-color: rgb(255, 127, 0);color:black;font-weight:bold\">word1: The first concept in the pair <span style=\"position:absolute;right:10em\">string</span> <br></p>\n",
    "    <p style=\"background-color: rgb(0, 255, 127);color:black;font-weight:bold\">word2: The second concept in the pair<span style=\"position:absolute;right:10em\">string</span> <br></p>\n",
    "    <p style=\"background-color: rgb(127, 0, 255);color:black;font-weight:bold\">POS: The majority part-of-speech of the concept words, as determined<br> by occurrence in the POS-tagged British National Corpus. Only pairs of <br>matching POS are included in SimLex-999<span style=\"position:absolute;right:10em;\">string</span> <br></p>\n",
    "    <p style=\"background-color: rgb(127, 127, 0);color:black;font-weight:bold\">SimLex999: The SimLex999 similarity rating. Note that average annotator <br>scores have been (linearly) mapped from the range [0,6] to the range [0,10]<br> to match other datasets such as WordSim-353 <span style=\"position:absolute;right:10em\">float</span> <br></p>\n",
    "    <p style=\"background-color: rgb(0, 127, 127);color:black;font-weight:bold\">conc(w1): The concreteness rating of word1 on a scale of 1-7. <br>Taken from the University of South Florida Free Association Norms database<span style=\"position:absolute;right:10em\">float</span> <br></p>\n",
    "    <p style=\"background-color: rgb(127, 0, 127);color:black;font-weight:bold\">conc(w2): The concreteness rating of word2 on a scale of 1-7. <br>Taken from the University of South Florida Free Association Norms database<span style=\"position:absolute;right:10em\">float</span> <br></p>\n",
    "    <p style=\"background-color: rgb(127, 127, 127);color:black;font-weight:bold\">concQ: The quartile the pair occupies based on the two concreteness ratings. <br>Used for some analyses in the above paper<span style=\"position:absolute;right:10em\">number</span> <br></p>\n",
    "    <p style=\"background-color: rgb(255, 127, 255);color:black;font-weight:bold\">Assoc(USF): The strength of free association from word1 to word2. <br>Values are taken from the University of South Florida Free Association Dataset<span style=\"position:absolute;right:10em\">float</span> <br></p>\n",
    "    <p style=\"background-color: rgb(255, 255, 127);color:black;font-weight:bold\">SimAssoc333: Binary indicator of whether the pair is one of the 333 <br>most associated in the dataset (according to Assoc(USF)). <br>This subset of SimLex999 is often the hardest for computational models<br> to capture because the noise from high association can confound the <br>similarity rating. See the paper for more details <span style=\"position:absolute;right:10em\">number</span> <br></p>\n",
    "    <p style=\"background-color: rgb(127, 255, 255);color:black;font-weight:bold\">SD(SimLex): The standard deviation of annotator scores when rating this pair. <br>Low values indicate good agreement between the 15+ annotators on the similarity value SimLex999. Higher scores indicate less certainty<span style=\"position:absolute;right:10em\">float</span> <br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Add libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import fasttext.util\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import kendalltau\n",
    "from gensim.models import KeyedVectors  \n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors \n",
    "import gensim.downloader as api\n",
    "from scipy.stats import kendalltau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Add dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>pos</th>\n",
       "      <th>simLex999</th>\n",
       "      <th>conc1</th>\n",
       "      <th>conc2</th>\n",
       "      <th>concQ</th>\n",
       "      <th>assoc</th>\n",
       "      <th>simAssoc</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>old</td>\n",
       "      <td>new</td>\n",
       "      <td>A</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smart</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>A</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard</td>\n",
       "      <td>difficult</td>\n",
       "      <td>A</td>\n",
       "      <td>8.77</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>cheerful</td>\n",
       "      <td>A</td>\n",
       "      <td>9.55</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard</td>\n",
       "      <td>easy</td>\n",
       "      <td>A</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2</td>\n",
       "      <td>5.82</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word1        word2 pos simLex999 conc1 conc2 concQ assoc simAssoc    sd\n",
       "0    old          new   A      1.58  2.72  2.81     2  7.25        1  0.41\n",
       "1  smart  intelligent   A       9.2  1.75  2.46     1  7.11        1  0.67\n",
       "2   hard    difficult   A      8.77  3.76  2.21     2  5.94        1  1.19\n",
       "3  happy     cheerful   A      9.55  2.56  2.34     1  5.85        1  2.18\n",
       "4   hard         easy   A      0.95  3.76  2.07     2  5.82        1  0.93"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simLex_data = pd.read_csv('SimLex-999.txt', sep='\\t', header=None)\n",
    "simLex_data.columns = ['word1', 'word2', 'pos','simLex999','conc1','conc2','concQ','assoc','simAssoc','sd']\n",
    "simLex_data = simLex_data.iloc[1:]\n",
    "simLex_data.reset_index(inplace=True,drop=True)\n",
    "simLex_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculate Wordnet similarity</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizePOS(df):\n",
    "    for ix in df.index:\n",
    "        df.at[ix,'pos'] = df.pos[ix].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWordnetSimilarity(df):\n",
    "    size = df.index.stop\n",
    "    word1 = df['word1'].to_numpy()\n",
    "    word2 = df['word2'].to_numpy()\n",
    "    pos = df['pos'].to_numpy()\n",
    "    path_sim = []\n",
    "    for i in range(size):\n",
    "        synset1 = wn.synsets(word1[i],pos[i])\n",
    "        synset2 = wn.synsets(word2[i],pos[i])\n",
    "        combinations = []\n",
    "        for synset_w1 in synset1:\n",
    "            for synset_w2 in synset2:\n",
    "                combinations.append(wn.path_similarity(synset_w1, synset_w2))\n",
    "        path_sim.append(max(combinations))\n",
    "    df['wordnetSimilarity'] = path_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>pos</th>\n",
       "      <th>simLex999</th>\n",
       "      <th>conc1</th>\n",
       "      <th>conc2</th>\n",
       "      <th>concQ</th>\n",
       "      <th>assoc</th>\n",
       "      <th>simAssoc</th>\n",
       "      <th>sd</th>\n",
       "      <th>wordnetSimilarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>old</td>\n",
       "      <td>new</td>\n",
       "      <td>a</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smart</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>a</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard</td>\n",
       "      <td>difficult</td>\n",
       "      <td>a</td>\n",
       "      <td>8.77</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>cheerful</td>\n",
       "      <td>a</td>\n",
       "      <td>9.55</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard</td>\n",
       "      <td>easy</td>\n",
       "      <td>a</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2</td>\n",
       "      <td>5.82</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word1        word2 pos simLex999 conc1 conc2 concQ assoc simAssoc    sd  \\\n",
       "0    old          new   a      1.58  2.72  2.81     2  7.25        1  0.41   \n",
       "1  smart  intelligent   a       9.2  1.75  2.46     1  7.11        1  0.67   \n",
       "2   hard    difficult   a      8.77  3.76  2.21     2  5.94        1  1.19   \n",
       "3  happy     cheerful   a      9.55  2.56  2.34     1  5.85        1  2.18   \n",
       "4   hard         easy   a      0.95  3.76  2.07     2  5.82        1  0.93   \n",
       "\n",
       "   wordnetSimilarity  \n",
       "0           0.333333  \n",
       "1           0.333333  \n",
       "2           1.000000  \n",
       "3           0.333333  \n",
       "4           0.333333  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizePOS(simLex_data)\n",
    "findWordnetSimilarity(simLex_data)\n",
    "simLex_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MissingWords</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does all words exist in wordnet? yes\n"
     ]
    }
   ],
   "source": [
    "print(\"Does all words exist in wordnet?\", \"no\" if (simLex_data['wordnetSimilarity'].isnull().sum()) else 'yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Looks like all the words exist is wordnet, however after my investigation I have found that some of the words actually do not exist in wordnet with their current pos. For example \"weird\" with pos \"a\" has another pos in wordnet. Looks like the path_similarity function automatically looks for a and s pos in the wordnet dataset</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Fast Text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMissing(missing, msg):\n",
    "    print(\"missing words are for \"+ msg+\":\")\n",
    "    for key in missing:\n",
    "        if(\"word1\" in missing[key]):\n",
    "            print(missing[key]['word1'])\n",
    "        if(\"word2\" in missing[key]):\n",
    "            print(missing[key]['word2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFastTextSimilarity(df):\n",
    "    cos_sims = []\n",
    "    missing = {} \n",
    "    ft = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "    all_words = ft.get_words()\n",
    "    print('Total number of words in the vocabulary:', len(all_words))\n",
    "    appended = False\n",
    "\n",
    "    for ix in df.index:\n",
    "        if (df['word1'][ix] not in all_words):\n",
    "            if(ix not in missing):\n",
    "                missing[ix] = {}\n",
    "            missing[ix][\"word1\"] = df['word1'][ix]\n",
    "            cos_sims.append(-1)    \n",
    "            appended = True\n",
    "        \n",
    "        if (df['word2'][ix] not in all_words):\n",
    "            if(ix not in missing):\n",
    "                missing[ix] = {}\n",
    "            missing[ix][\"word2\"] = df['word2'][ix]\n",
    "            if(not appended):\n",
    "                cos_sims.append(-1)     \n",
    "            appended = True\n",
    "        if appended:\n",
    "            continue\n",
    "        word1_vec = ft.get_word_vector(df['word1'][ix])\n",
    "        word2_vec = ft.get_word_vector(df['word2'][ix])\n",
    "        cos_sims.append(1 - cosine(word1_vec, word2_vec)) \n",
    "    printMissing(missing, 'fastTextSimilarity')\n",
    "\n",
    "    df['fastTextSimilarity'] = cos_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWupSimilarity(df):\n",
    "    wupS = []\n",
    "    missing = {}\n",
    "\n",
    "    for ix in df.index:\n",
    "        wn_pos = None\n",
    "        if df['pos'][ix] == 'n':\n",
    "            wn_pos = wn.NOUN\n",
    "        elif df['pos'][ix] == 'v':\n",
    "            wn_pos = wn.VERB\n",
    "        else:\n",
    "            wn_pos = wn.ADJ\n",
    "        word1_pos_synsets = [s for s in wn.synsets(df['word1'][ix], pos=wn_pos)]\n",
    "        word2_pos_synsets = [s for s in wn.synsets(df['word2'][ix], pos=wn_pos)]\n",
    "        appended = False\n",
    "\n",
    "        if(len(word1_pos_synsets) == 0):\n",
    "            if(ix not in missing):\n",
    "                missing[ix] = {}\n",
    "            missing[ix][\"word1\"] = df['word1'][ix]\n",
    "            wupS.append(-1)\n",
    "            appended = True\n",
    "        if (len(word2_pos_synsets) == 0):\n",
    "            if(ix not in missing):\n",
    "                missing[ix] = {}\n",
    "            missing[ix][\"word2\"] = df['word2'][ix]\n",
    "            if(not appended):\n",
    "                wupS.append(-1)\n",
    "            appended = True\n",
    "        if appended: \n",
    "            continue\n",
    "\n",
    "        combinations = []\n",
    "        for synset_w1 in word1_pos_synsets:\n",
    "            for synset_w2 in word2_pos_synsets:\n",
    "                combinations.append(wn.wup_similarity(synset_w1, synset_w2))\n",
    "        wupS.append(max(combinations))\n",
    "            \n",
    "            \n",
    "    printMissing(missing, 'wupSimilarity')\n",
    "\n",
    "    df['wupSimilarity'] = wupS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findGensimSimilarity(df):\n",
    "    corpus = api.load('text8')  \n",
    "    model = Word2Vec(corpus)    \n",
    "    gensimS = []\n",
    "    missing = {}\n",
    "    for ix in df.index:\n",
    "        appended = False\n",
    "        try:\n",
    "            vec1 = model.wv[df['word1'][ix]]\n",
    "        except KeyError:\n",
    "            if(ix not in missing):\n",
    "                missing[ix] = {}\n",
    "            missing[ix][\"word1\"] = df['word1'][ix]\n",
    "            gensimS.append(-1)\n",
    "            appended = True\n",
    "        finally:\n",
    "            try:\n",
    "                vec2 = model.wv[df['word2'][ix]]\n",
    "            except KeyError:\n",
    "                if(ix not in missing):\n",
    "                    missing[ix] = {}\n",
    "                if(\"word2\" not in missing[ix]):\n",
    "                    missing[ix][\"word2\"] = df['word2'][ix]\n",
    "                if (not appended):\n",
    "                    gensimS.append(-1)\n",
    "                appended = True\n",
    "        if (appended):\n",
    "            continue\n",
    "        similarity = 1 - cosine(vec1, vec2)\n",
    "        gensimS.append(similarity)\n",
    "\n",
    "\n",
    "\n",
    "    printMissing(missing, 'gensimSimilarity')\n",
    "    df['gensimSimilarity'] = gensimS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing words are for wupSimilarity:\n",
      "missing words are for gensimSimilarity:\n",
      "hallway\n",
      "suds\n",
      "orthodontist\n",
      "orthodontist\n",
      "hallway\n",
      "hallway\n",
      "disorganize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the vocabulary: 2000000\n",
      "missing words are for fastTextSimilarity:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>pos</th>\n",
       "      <th>simLex999</th>\n",
       "      <th>conc1</th>\n",
       "      <th>conc2</th>\n",
       "      <th>concQ</th>\n",
       "      <th>assoc</th>\n",
       "      <th>simAssoc</th>\n",
       "      <th>sd</th>\n",
       "      <th>wordnetSimilarity</th>\n",
       "      <th>wupSimilarity</th>\n",
       "      <th>gensimSimilarity</th>\n",
       "      <th>fastTextSimilarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>old</td>\n",
       "      <td>new</td>\n",
       "      <td>a</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.382462</td>\n",
       "      <td>0.441964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smart</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>a</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.311402</td>\n",
       "      <td>0.704955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard</td>\n",
       "      <td>difficult</td>\n",
       "      <td>a</td>\n",
       "      <td>8.77</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.609987</td>\n",
       "      <td>0.631380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>cheerful</td>\n",
       "      <td>a</td>\n",
       "      <td>9.55</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.476049</td>\n",
       "      <td>0.545871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard</td>\n",
       "      <td>easy</td>\n",
       "      <td>a</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2</td>\n",
       "      <td>5.82</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.685552</td>\n",
       "      <td>0.486345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word1        word2 pos simLex999 conc1 conc2 concQ assoc simAssoc    sd  \\\n",
       "0    old          new   a      1.58  2.72  2.81     2  7.25        1  0.41   \n",
       "1  smart  intelligent   a       9.2  1.75  2.46     1  7.11        1  0.67   \n",
       "2   hard    difficult   a      8.77  3.76  2.21     2  5.94        1  1.19   \n",
       "3  happy     cheerful   a      9.55  2.56  2.34     1  5.85        1  2.18   \n",
       "4   hard         easy   a      0.95  3.76  2.07     2  5.82        1  0.93   \n",
       "\n",
       "   wordnetSimilarity  wupSimilarity  gensimSimilarity  fastTextSimilarity  \n",
       "0           0.333333            0.5          0.382462            0.441964  \n",
       "1           0.333333            0.5          0.311402            0.704955  \n",
       "2           1.000000            1.0          0.609987            0.631380  \n",
       "3           0.333333            0.5          0.476049            0.545871  \n",
       "4           0.333333            0.5          0.685552            0.486345  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findWupSimilarity(simLex_data)\n",
    "findGensimSimilarity(simLex_data)\n",
    "findFastTextSimilarity(simLex_data)\n",
    "simLex_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findKendall(df):\n",
    "    scores_df = df[[\"wordnetSimilarity\", \"fastTextSimilarity\", \"wupSimilarity\", \"gensimSimilarity\",\"simLex999\"]]\n",
    "\n",
    "\n",
    "    # df = scores_df[(scores_df != -1).all(axis=1)]\n",
    "\n",
    "    kendall_wordnet, _ = kendalltau(df[\"wordnetSimilarity\"], df[\"simLex999\"])\n",
    "    kendall_ft, _ = kendalltau(df[\"fastTextSimilarity\"], df[\"simLex999\"])\n",
    "    kendall_wu_palmer, _ = kendalltau(df[\"wupSimilarity\"], df[\"simLex999\"])\n",
    "    kendall_gensim, _ = kendalltau(df[\"gensimSimilarity\"], df[\"simLex999\"])\n",
    "\n",
    "    print(\"Kendall's Tau for Wordnet:\", kendall_wordnet)\n",
    "    print(\"Kendall's Tau for FastText:\", kendall_ft)\n",
    "    print(\"Kendall's Tau for Wu-Palmer:\", kendall_wu_palmer)\n",
    "    print(\"Kendall's Tau for Gensim:\", kendall_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendall's Tau for Wordnet: 0.35344887126870356\n",
      "Kendall's Tau for FastText: 0.3301400933912036\n",
      "Kendall's Tau for Wu-Palmer: 0.32114437927102696\n",
      "Kendall's Tau for Gensim: 0.16782692636713945\n"
     ]
    }
   ],
   "source": [
    "findKendall(simLex_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Results show that wordnet has the highest kendall's tau coefficient which makes it the best among 4 given similarity models. However I bilieve that fastText could do better, but due to my network I did not download all the files.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
